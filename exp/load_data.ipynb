{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDownloading emoji data ...\u001b[0m\n",
      "\u001b[92m... OK\u001b[0m (Got response in 0.22 seconds)\n",
      "\u001b[33mWriting emoji data to C:\\Users\\victo\\.demoji/codes.json ...\u001b[0m\n",
      "\u001b[92m... OK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords \n",
    "import demoji\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "import spacy\n",
    "stop_words = set(stopwords.words('english')) \n",
    "demoji.download_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gg2020.json', encoding='utf8') as json_file:\n",
    "    data = [json.loads(line) for line in json_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('gg2013.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_all_entities(text):\n",
    "    entity_prefixes = ['@','#']\n",
    "    for separator in  string.punctuation:\n",
    "        if separator not in entity_prefixes :\n",
    "            text = text.replace(separator,' ')\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        word = word.strip()\n",
    "        if word:\n",
    "            if word[0] not in entity_prefixes:\n",
    "                words.append(word)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = []\n",
    "count = 0\n",
    "for tweet in data:\n",
    "    tweet_text = tweet['text']\n",
    "    no_http = re.sub('http://\\S+|https://\\S+', '', tweet_text)\n",
    "    remove_tag = strip_all_entities(no_http)\n",
    "    #emoji_list = demoji.findall(remove_tag)\n",
    "    #tweet_token = nltk.word_tokenize(remove_tag)\n",
    "    #filtered_tweet = tweet_token\n",
    "    #filtered_tweet = [w for w in filtered_tweet if not w in emoji_list]\n",
    "    #filtered_tweet = [w for w in filtered_tweet if not w in string.punctuation]\n",
    "    #filtered_tweet = [w for w in filtered_tweet if w.isalnum()]\n",
    "    #filtered_tweet = [w.lower() for w in filtered_tweet]\n",
    "    clean_data.append(no_http)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "clean_data.sort()\n",
    "unique_clean_data = list(set(clean_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127315\n",
      "174643\n"
     ]
    }
   ],
   "source": [
    "print(len(unique_clean_data))\n",
    "print(len(clean_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127315\n"
     ]
    }
   ],
   "source": [
    "num_k = []\n",
    "for elem in clean_data:\n",
    "    if elem not in num_k:\n",
    "        num_k.append(elem)\n",
    "print(len(num_k))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find hosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search host\n",
    "host_list = ['host', 'hosts', 'hosting', 'hosted']\n",
    "host_tweet = []\n",
    "for tweet in clean_data:\n",
    "    for test_word in host_list:\n",
    "        if test_word in tweet:\n",
    "            host_tweet.append(tweet)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: parsing empty text\n"
     ]
    }
   ],
   "source": [
    "tag = nltk.pos_tag(clean_data[0])\n",
    "pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "cp = nltk.RegexpParser(pattern)\n",
    "cs = cp.parse(tag)\n",
    "iob = tree2conlltags(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(' '.join(word for word in clean_data[i]))\n",
    "print(' '.join(word for word in clean_data[i]))\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = []\n",
    "for tweet in host_tweet:\n",
    "    doc = nlp(tweet)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            name_list.append(ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_list = list(set(name_list))\n",
    "counter = []\n",
    "for item in unique_list:\n",
    "    counter.append(name_list.count(item))\n",
    "sort_counter = sorted(range(len(counter)), key=lambda k: counter[k], reverse=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_list = []\n",
    "for i in range(5):\n",
    "    name = unique_list[sort_counter[i]]\n",
    "    possible_list.append(name)\n",
    "reduced_possible_list = []\n",
    "for i in range(5):\n",
    "    test = possible_list[i]\n",
    "    conflict = 0\n",
    "    for j in range(5):\n",
    "        if test in possible_list[j] and i != j:\n",
    "            conflict = 1\n",
    "            break\n",
    "    if test not in reduced_possible_list and conflict == 0:\n",
    "        reduced_possible_list.append(test)\n",
    "# search first\n",
    "hosts = []\n",
    "hosts.append(reduced_possible_list[0])\n",
    "host_idx = possible_list.index(hosts[0])\n",
    "max_num = counter[sort_counter[host_idx]]\n",
    "# search second\n",
    "name_pair = []\n",
    "for name1 in reduced_possible_list:\n",
    "    for name2 in reduced_possible_list:\n",
    "        count = 0\n",
    "        for tweet in host_tweet:\n",
    "            tweetstr = ' '.join(map(str, tweet)) \n",
    "            if name1 + ' and ' + name2 in tweetstr or name2 + ' and ' + name1 in tweetstr:\n",
    "                count += 1\n",
    "        if count > max_num * 0.1:\n",
    "            name_pair.append([name1, name2, count])\n",
    "name_pair.sort(key=lambda x: x[2], reverse=True)\n",
    "if len(name_pair) > 0:\n",
    "    hosts = [name_pair[0][0],name_pair[0][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Will Ferrell']\n"
     ]
    }
   ],
   "source": [
    "print(hosts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find awards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awards_bag_of_words = ['-','performance', 'actress', 'actor', 'supporting', 'role', 'director', 'motion', 'picture', 'drama','animated', 'film', 'song','comedy', 'musical', 'language', 'foreign','screenplay', 'orginal', 'television', 'tv', 'series', 'mini-series', 'mini']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'(?<=w[oi]n[s]\\s)[Bb]est.*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_sep = ['for','who',' s ','made',' | ', 'with' ,' at', ' http', ' #', '(','.', ',', '!', '?','\\\\', ':', ';', '\"', \"'\",'the','but','although', 'made']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in extracted_text_data:\n",
    "    award = re.findall(pattern, tweet)\n",
    "    if award:\n",
    "        for a in award:\n",
    "            a = a.lower()\n",
    "            for sep in tweet_sep:\n",
    "                if sep in a:\n",
    "                    a = a.split(sep)[0]\n",
    "            #if \"/\" in a:\n",
    "            #    a = a.replace('/',' or ')\n",
    "            awards.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "awards_list = {}\n",
    "for tweet in clean_data:\n",
    "    if \"best\" in tweet and \"performance\" in tweet and \"award\" in tweet:\n",
    "        collect = []\n",
    "        for word in tweet:\n",
    "            if word == \"best\" and len(collect) == 0:\n",
    "                collect.append(word)\n",
    "            elif word == \"performance\" and len(collect) == 1:\n",
    "                collect.append(word)  \n",
    "            elif word == \"by\" and len(collect) == 2:\n",
    "                collect.append(word)                 \n",
    "            elif word == \"award\" and len(collect) > 3:\n",
    "                collect.append(word)\n",
    "                collect_str = ' '.join(map(str, collect))\n",
    "                if collect_str not in awards_list:\n",
    "                    awards_list[collect_str] = 0\n",
    "                awards_list[collect_str] += 1\n",
    "                collect = []\n",
    "            elif len(collect) >= 3:\n",
    "                collect.append(word)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best performance by an actor in a motion picture drama last night s ceremony was the first major award': 1, 'best performance by an actor in a motion picture musical or comedy by golden globe award': 1, 'best performance by an actor in a supporting role in any motion picture by golden globe award': 1, 'best performance by an actor award': 1, 'best performance by an actor in a motion picture drama by golden globe award': 1, 'best performance by an actress in a supporting role award': 1, 'best performance by an actor in a limited series or motion picture made for television award': 1, 'best performance by an actor in a supporting role in a motion picture this is pacino s 18th nomination with 4 wins and the cecil b demille award': 1, 'best performance by an actor in a limited series because it s jharrel s award': 1, 'best performance by an actor in a motion picture drama upon accepting his award': 1}\n"
     ]
    }
   ],
   "source": [
    "print(awards_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "awards_list = {}\n",
    "for tweet in clean_data:\n",
    "    if \"the\" in tweet and \"award\" in tweet:\n",
    "        #print(tweet)\n",
    "        collect = []\n",
    "        for word in list(reversed(tweet)):  \n",
    "            if word == \"award\":\n",
    "                collect.insert(0,word)\n",
    "            elif word == \"the\" and len(collect) > 0:\n",
    "                #print(collect_str)\n",
    "                collect_str = ' '.join(map(str, collect))\n",
    "                if len(collect) > 2 and len(collect) <= 4 and 'golden' not in collect_str:\n",
    "                    doc = nlp(collect_str)\n",
    "                    for ent in doc.ents:\n",
    "                        if ent.label_ == \"PERSON\":                    \n",
    "                            if collect_str not in awards_list:\n",
    "                                awards_list[collect_str] = 0\n",
    "                            awards_list[collect_str] += 1\n",
    "                            break\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "                collect = []\n",
    "            elif len(collect) > 0:\n",
    "                collect.insert(0,word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'brightest genius award': 1, 'carol burnett award': 109, 'cecille b demille award': 1, 'carroll burnett award': 2, 'prestigious carol burnett award': 1, 'chairman s award': 1, 'carol burnett show award': 1, 'green goddess award': 1, 'event carol burnett award': 1, 'galton an award': 1, 'honorary carol burnett award': 1, 'carol burnet award': 1, 'inaugural carol burnett award': 1, 'jussie smollett award': 1, 'director s guild award': 1, 'official kickoff to award': 1}\n"
     ]
    }
   ],
   "source": [
    "print(awards_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in clean_data:\n",
    "    tweetstr = ' '.join(map(str, tweet)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp-env)",
   "language": "python",
   "name": "nlp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
